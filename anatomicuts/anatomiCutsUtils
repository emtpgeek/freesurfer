#!/usr/bin/env python

import csv
import os
import math
from scipy import stats
import numpy as np
import nibabel as nib
import os.path
import scipy.spatial
import glob
import subprocess
import pandas as pd
from nipy.modalities.fmri.glm import GeneralLinearModel
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
import freesurfer as fs
import argparse

def getCorrespondingClusters(correspondance,order=True):
		corr=dict()
		distances=dict()
		indeces=dict()
		ind=0
		with open(correspondance, 'r') as csvfile:
				corReader= csv.reader(csvfile, delimiter=',', quotechar='|')	
				header=True
				for row in corReader:
					if not header and len(row)>1:
						if order:
							corr[row[1]]=row[0]
							indeces[ind]=row[1]
							distances[row[1]]=float(row[2])
						else:
							corr[row[0]]=row[1]
							indeces[ind]=row[0]
							distances[row[0]]=float(row[2])
						ind+=1
					else:
						header=False
		return corr, distances, indeces
def thicknessPerStructure(classificationFile, classification_cols, target_subject, subjects_dir, groupA, groupB, lenght=45, std=5,clustersToAnalyze=[200], model="DTI", delimiter="," , groups=[0,1], thickness=False):
	ac_folder=f"dmri.ac/{lenght}/{std}"
	clusterNum=200
	lut = fs.LookupTable.read("/usr/local/freesurfer/dev/FreeSurferColorLUT.txt")
	print(lut[28].name)

	indeces=[]
	groups_cat = {row[classification_cols[0]] :row[classification_cols[1]] for _, row in pd.read_csv(classificationFile, delimiter=delimiter).iterrows()}
	print(groups_cat)
	significant_childs=set()
	thicknesses=[[],[],[],[]]

	for j in range(len(thicknesses)):
		for i in range(200):
			thicknesses[j].append(dict())

	ww=0
	for s , cat2 in groups_cat.items():
	
		if cat2 > 10:
			cat = 0
		else:
			cat =  cat2
		print(s,cat)
		gres = glob.glob(f'{subjects_dir}/{s}*')
		if len(gres)>0 and cat< len(thicknesses):
			subject=gres[0].split("/")[-1]
			try:
				correspondences= f"{subjects_dir}/{subject}/{ac_folder}/match/{target_subject}_{subject}_c{clusterNum}_hungarian.csv"
				for i in range(clusterNum):
					corr, distances, indeces =  getCorrespondingClusters(correspondences, False)
					values=pd.read_csv(f"{subjects_dir}/{subject}/{ac_folder}/measures/surf/"+corr[indeces[i]]+".csv")
					#print(values.shape[0])
					for j in range(values.shape[0]):
						if values['label.start'][j] in thicknesses[cat][i]:
							thicknesses[cat][i][values['label.start'][j]].append(values['thickness.start'][j])
						else:
							thicknesses[cat][i][values['label.start'][j]]= [values['thickness.start'][j]]

						if values['label.end'][j] in thicknesses[cat][i]:
							thicknesses[cat][i][values['label.end'][j]].append(values['thickness.end'][j])
						else:
							thicknesses[cat][i][values['label.end'][j]]= [values['thickness.end'][j]]

			except Exception as e:
				print("ERROR",e)
		ww+=1

	try:

		for i in range(200):
			total =0
			plt.figure()
			print("hola")
			for k,v in thicknesses[1][i].items():
				total += len(v)
			toplot=[]
			labels=[]
			colors=[]
			for k,v in thicknesses[1][i].items():
				
				if  len(v)*100 /total > 5 and k in thicknesses[1][i] and k in thicknesses[2][i]  and np.mean(v) >1 and str(k).isnumeric() :
					print("plot", len(v)*100/total)
					toplot.append(v)				
					toplot.append(thicknesses[1][i][k])				
					toplot.append(thicknesses[2][i][k])				
					labels.append(lut[int(k)].name)
					labels.append("")
					labels.append("")
					print(lut[int(k)].name , int(k), k)
					colors.append('r')
					colors.append('b')
					colors.append('g')

			if len(toplot) >0:
				parts =	plt.violinplot(toplot)
				for v , pc in enumerate(parts['bodies']):
					pc.set_facecolor(colors[v])
					pc.set_edgecolor(colors[v])
					pc.set_alpha(1)
				plt.gcf().set_size_inches(len(labels),5)	
				plt.xticks(range(1,len(labels)+1), labels)
				plt.savefig(f"{subjects_dir}/average/{i}.png")
	except Exception as e :
		print(e)
	#bash /space/erebus/2/users/vsiless/code/freesurfer/anatomicuts/dmri_ac.sh GA BAKP64e_nrecon-trio3_vb17 40 5 /space/snoke/1/public/vivros/hd_tracula/labels_hd.csv  0:2 999999999 2 [0,1,2] True)

def averageCorrespondingClusters(correspondences, imagesFolder, outputFolder,  clusterIndeces):
		averages=dict()
		norm=dict()

		for s_i, correspondance in enumerate(correspondences):
				try:
					for clusterIndex in clusterIndeces:
						corr, distances, indeces =  getCorrespondingClusters(correspondance, True)
						#print(correspondance)
						image=imagesFolder[s_i]+""+indeces[clusterIndex]+".nii.gz"
						im =nib.load(image)
						b= im.get_data()
						b = b/b.max()
						b = np.ceil(b) 
						if clusterIndex in averages:
							b += averages[clusterIndex].get_data() 
							averages[clusterIndex]=nib.Nifti1Image(b, averages[clusterIndex].get_affine())
							norm[clusterIndex]+=1
						else:
							averages[clusterIndex] = nib.Nifti1Image(b, im.get_affine())    
							norm[clusterIndex]=1
				except Exception as e:
					print(str(e))

		for clusterIndex in clusterIndeces:
				directory=outputFolder
				if not os.path.exists(directory):
					os.makedirs(directory)
				data=averages[clusterIndex].get_data()/norm[clusterIndex]
				nib.Nifti1Image(data, averages[clusterIndex].get_affine()).to_filename(directory+"/"+str(clusterIndex)+'.nii.gz')
				print ("saving",directory+"/"+str(clusterIndex)+'.nii.gz')

def readTree(numNodes, histogramFile,header=True):
    almostFoundAllClusters=False
    foundAllClusters=False
    nodes_childs=dict()
    whos_dad=dict()
    
    with open(histogramFile, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=',', quotechar='|')	
        clusters=set()

        for row in reader:
            if not header:
                if not foundAllClusters:
                    try:
                        if row[0] in clusters:
                            clusters.remove(row[0])
                        clusters.add(row[1])
                        if len(clusters)==numNodes :
                            foundAllClusters=True
                            for i in clusters:
                                nodes_childs[i]=[]
                            
                    except:
                        #print ("header")
                        None
                else:
                    if row[0] in whos_dad :
                        dad = whos_dad[row[0]]
                        if row[0] in nodes_childs[dad]:
                            nodes_childs[dad].remove(row[0])
                        nodes_childs[dad].append(row[1])
                        whos_dad[row[1]]=dad	
                    else:
                        nodes_childs[row[0]].append(row[1])
                        whos_dad[row[1]]=row[0]
            else:
                header=False
            
    return nodes_childs, whos_dad

def groupAnalysis( headers, cols , groups_classification, classification_columns, clustersToAnalyze, subjects_dir, target_subject, lenght, std, delimiter=",", groupA=[0], groupB=[1], thickness=False):
	ac_folder=f"dmri.ac/{lenght}/{std}"
	

	indeces=[]
	groups_cat = {row[classification_columns[0]] :row[classification_columns[1]] for _, row in pd.read_csv(groups_classification, delimiter=delimiter).iterrows()}
	print(groups_cat)

	significant_childs=set()
	for c_i, clusterNum in enumerate(clustersToAnalyze):
		
		childs, dads = readTree(clusterNum, f"{subjects_dir}/{target_subject}/{ac_folder}/HierarchicalHistory.csv")
		#print(childs, dads)
		order_nodes= pd.read_csv(f"{subjects_dir}/{target_subject}/{ac_folder}/measures/{target_subject}_{target_subject}_c{clusterNum}.csv",delimiter=",", header=0,usecols=[0])
		#print(order_nodes["Cluster"][0])
		ys=[]
		for a in headers:
			ys.append([])
			
		for i in range(clusterNum):
			for j in range(len(headers)):
				ys[j].append([])
		X=[]
		
		for s in groups_cat.keys():
			#print(f'{subjects_dir}/{s}*')
			gres = glob.glob(f'{subjects_dir}/{s}*')
			if len(gres)>0:
				subject=gres[0].split("/")[-1]
				if thickness:
					measures=f"{subjects_dir}/{subject}/{ac_folder}/measures/surfaceMeasures.csv"
				else:
					measures=f"{subjects_dir}/{subject}/{ac_folder}/measures/{target_subject}_{subject}_c{clusterNum}.csv"
				if os.path.exists(measures):
					data = pd.read_csv(measures,delimiter=",", header=0, names=headers, usecols=cols)
					#print(measures, headers, cols)
					if len(data[headers[0]])>= clusterNum:
						#print(int(groups_cat[s]	), groupA, groupB)
						if int(groups_cat[s]) in groupA:
							X=np.append(X,[1, 0])
			
						elif int(groups_cat[s]) in groupB:
							X=np.append(X,[0, 1])
				
						if int(groups_cat[s]) in np.concatenate( (groupA,groupB), axis=None):
							#print("hola")
							for i,h in enumerate(headers):
								for j in range(clusterNum):
									ys[i][j].append(data[h][j])

		#print(X, ys)
		X= np.array(X).reshape(len(ys[0][0]),2)
		for i, m  in enumerate(headers):
			Y=ys[i][0]
			for j in range(1,len(ys[i])):
				#print(np.shape(ys[i][j]))
				Y = np.vstack((Y,[ys[i][j]]))
			
			Y=np.array(Y).transpose()
			#print(np.shape(Y))
			cval = np.hstack((-1, 1))
			model = GeneralLinearModel(X)
			model.fit(Y)
			p_vals = model.contrast(cval).p_value() # z-transformed statistics
			#print( p_vals)
			for index, p in enumerate(p_vals):
				if p*len(ys[0]) <0.05:
					if len(childs[str(order_nodes["Cluster"][index])]) ==0:
						significant_childs.add(str(order_nodes["Cluster"][index]))
					else:
						for c in childs[str(order_nodes["Cluster"][index])]:
							significant_childs.add(c)
					
				if clusterNum == clustersToAnalyze[-1] and p<0.05 and str(order_nodes["Cluster"][index]) in significant_childs:
					print(m,index,p, p,str(order_nodes["Cluster"][index]))
					indeces.append(index)
					
				 
			cval = np.hstack((1, -1))
			model = GeneralLinearModel(X)
			model.fit(Y)
			p_vals = model.contrast(cval).p_value() # z-transformed statistics
			#print( p_vals)
			for index, p in enumerate(p_vals):
				if p*len(ys[0]) <0.05:
					#print(index,p, p*len(ys[0])*4,str(order_nodes["Cluster"][index])) 
					if len(childs[str(order_nodes["Cluster"][index])]) ==0:
						significant_childs.add(str(order_nodes["Cluster"][index]))
					else:
						for c in childs[str(order_nodes["Cluster"][index])]:
							significant_childs.add(c)
		    #plt.show()
				if clusterNum == clustersToAnalyze[-1] and p<0.05 and str(order_nodes["Cluster"][index]) in significant_childs:
					print(m, index,p, p,str(order_nodes["Cluster"][index]))
					indeces.append(index)
	return indeces
					

def plotAverageMeasures( headers, cols , groups_classification, classification_columns, clustersToAnalyze, subjects_dir, target_subject, lenght, std, delimiter=",", groups=[[0],[1],[2],[3]], thickness=False):
	groups_cat = {row[classification_columns[0]] : row[classification_columns[1]] for _, row in pd.read_csv(groups_classification, delimiter=delimiter).iterrows()}
	clusterNum=200
	order_nodes= pd.read_csv(f"{subjects_dir}/{target_subject}/dmri.ac/{lenght}/{std}/measures/{target_subject}_{target_subject}_c{clusterNum}.csv",delimiter=",", header=0,usecols=[0])
	measures=[]
	for g in groups:
		measures.append([])
	for gi,g in enumerate(groups):
		for a in headers:
			measures[gi].append([])
		
	for gi, g in enumerate(groups):
		for i in range(clusterNum):
			for j in range(len(headers)):
				measures[gi][j].append([])
	
	for s in groups_cat.keys():
		gres = glob.glob(f'{subjects_dir}/{s}*')
		if len(gres)>0:
			subject=gres[0].split("/")[-1]
			if thickness:
				measuresFile=f"{subjects_dir}/{subject}/dmri.ac/{lenght}/{std}/measures/surfaceMeasures.csv"
			else:
				measuresFile=f"{subjects_dir}/{subject}/dmri.ac/{lenght}/{std}/measures/{target_subject}_{subject}_c{clusterNum}.csv"
			try:
				data = pd.read_csv(measuresFile,delimiter=",", header=0, names=headers, usecols=cols)
				#print(measures)
				if len(data[headers[0]])>= clusterNum:
					val  = [ i for i in range(len(groups))  if int(groups_cat[s]) in groups[i]]
					if len(val)>0:
						group=val[0]
						for i,h in enumerate(headers):
							for j in range(clusterNum):
								if not math.isnan(data[h][j]):
									measures[group][i][j].append(data[h][j])
			except Exception as e:
				print (e, measuresFile)
	for j, h in enumerate(headers):
		for i in clustersToAnalyze:
			plt.figure()
			plt.title(headers[j])
			#print(headers[j])
			for gi, g in enumerate( groups):
				#print(measures[gi][j][i] ,[gi])
				plt.violinplot(measures[gi][j][i], [gi],  showmeans=True )
				#print("dwhola")
			plt.savefig(f"{subjects_dir}/average/dmri.ac/{lenght}_{std}/"+str(i)+"_"+headers[j]+".png")		
	#plt.show()
def plotScatter( headers, cols , groups_classification, classification_columns, clustersToAnalyze, subjects_dir, target_subject, lenght, std, delimiter=",", groups=[[0],[1],[2],[3]], thickness=False):
	groups_cat = {row[classification_columns[0]] : row[classification_columns[1]] for _, row in pd.read_csv(groups_classification, delimiter=delimiter).iterrows()}
	clusterNum=200
	order_nodes= pd.read_csv(f"{subjects_dir}/{target_subject}/dmri.ac/{lenght}/{std}/measures/{target_subject}_{target_subject}_c{clusterNum}.csv",delimiter=",", header=0,usecols=[0])
	measures=[]
	for g in groups:
		measures.append([])
	for gi,g in enumerate(groups):
		for a in headers:
			measures[gi].append([])
		
	for gi, g in enumerate(groups):
		for i in range(clusterNum):
			for j in range(len(headers)):
				measures[gi][j].append([])
	
	for s in groups_cat.keys():
		gres = glob.glob(f'{subjects_dir}/{s}*')
		if len(gres)>0:
			subject=gres[0].split("/")[-1]
			if thickness:
				measuresFile=f"{subjects_dir}/{subject}/dmri.ac/{lenght}/{std}/measures/surfaceMeasures.csv"
			else:
				measuresFile=f"{subjects_dir}/{subject}/dmri.ac/{lenght}/{std}/measures/{target_subject}_{subject}_c{clusterNum}.csv"
			try:
				data = pd.read_csv(measuresFile,delimiter=",", header=0, names=headers, usecols=cols)
				#print(measures)
				if len(data[headers[0]])>= clusterNum:
					val  = [ i for i in range(len(groups))  if int(groups_cat[s]) in groups[i]]
					if len(val)>0:
						group=val[0]
						for i,h in enumerate(headers):
							for j in range(clusterNum):
								if not math.isnan(data[h][j]):
									measures[group][i][j].append(data[h][j])
			except Exception as e:
				print (e, measuresFile)
	for j, h in enumerate(headers):
		for i in clustersToAnalyze:
			plt.figure()
			plt.title(headers[j])
			#print(headers[j])
			todo=[]
			for gi, g in enumerate( groups):
				#print(measures[gi][j][i] ,[gi])
				todo.append	(measures[gi][j][i])
			plt.boxplot( todo )
			#print("dwhola")
			plt.savefig(f"{subjects_dir}/average/dmri.ac/{lenght}_{std}/"+str(i)+"_"+headers[j]+"_scatter.png")		
			plt.close()
	#plt.show()

def GA(classificationFile, classification_cols, target_subject, subjects_dir, groupA, groupB, lenght=45, std=5,clustersToAnalyze=[200], model="DTI", delimiter="," , groups=[0,1], thickness=False):
	measures=['FA','MD','RD','AD']
	if model == 'DKI':
		measures=measures+['MK','RK','AK']

	indeces = []	
	headers = []
	columns = []


	#indeces += groupAnalysis(headers=["N"],cols=[1], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=clustersToAnalyze,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, groupA=groupA, groupB=groupB, lenght=lenght, std=std)
	#headers+= ["N"]
	#columns+=[1]

	for i, m in enumerate(measures):
		print("mean"+m, " " , 2+4*i, classification_cols)
		headers += [" mean"+m+" "]
		columns+=[2+i*4]
		indeces += groupAnalysis(headers=[" mean"+m+" "],cols=[2+i*4], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=clustersToAnalyze,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, groupA=groupA, groupB=groupB, lenght=lenght, std=std)
		#plotAverageMeasures(headers=["mean"+m],cols=[2+i*4], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std) #groups=[groupA,groupB], 

	
	for i, m in enumerate(measures):
		headers += ["   meanCentroid"+m+" "]
		columns+=[4+i*4]
		print("meanCentroid"+m, " " , 2+4*i, classification_cols)
		indeces += groupAnalysis(headers=["   meanCentroid"+m+" "],cols=[4+i*4], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=clustersToAnalyze,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, groupA=groupA, groupB=groupB, lenght=lenght, std=std)
		#plotAverageMeasures(headers=["meanCentroid"+m],cols=[4+i*4], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std) #groups=[groupA,groupB], 
		plotScatter(headers=["meanCentroid"+m],cols=[4+i*4], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std) #groups=[groupA,groupB], 
	
	if thickness:
		measures=["curv.star","curv.end","thickness.start", "thickness.end"]

		for i, m in enumerate(measures):
			indeces += groupAnalysis(headers=[m],cols=[1+i], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=clustersToAnalyze,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, groupA=groupA, groupB=groupB, lenght=lenght, std=std, thickness=thickness)

			#plotAverageMeasures(headers=measures,cols=[0,1,2,3], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std, groups=groups, thickness=True) #groups=[groupA,groupB], 
			plotScatter(headers=measures,cols=[0,1,2,3], groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std, groups=groups, thickness=True) #groups=[groupA,groupB], 
	print(columns, headers,indeces)
	#plotAverageMeasures(headers=headers,cols=columns, groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std, groups=groups, thickness=False) #groups=[groupA,groupB], 
	plotScatter(headers=headers,cols=columns, groups_classification=classificationFile, classification_columns=classification_cols,clustersToAnalyze=indeces,target_subject=target_subject,subjects_dir=subjects_dir, delimiter=delimiter, lenght=lenght, std=std, groups=groups, thickness=False) #groups=[groupA,groupB], 




cta=[200]
parser = argparse.ArgumentParser()
# Required
parser.add_argument('-f','--function',  required=True, help='Function to use: GA for group analysis.')
parser.add_argument('-m','--model',required=False,  help='Model, which could be DTI or DKI')
parser.add_argument('-cf','--classification_file', metavar='file', required=False, help='classification file')
parser.add_argument('-co','--corresponding_file_list',  required=False, help='corresponding file list')
parser.add_argument('-if','--imagesFolder', required=False, help='images folder list')
parser.add_argument('-of','--outputFolder', required=False, help='output folder')
parser.add_argument('-in','--clusterIndeces',required=False, help='input clusters')
parser.add_argument('-cc','--classification_columns',required=False,  help='classification columns')
parser.add_argument('-cta','--clusters_to_analyze',required=False,  help='Clusters to analyze')
parser.add_argument('-ts','--target_subject',required=False,  help='target subject')
parser.add_argument('-s','--subjects_folder', required=False, help='subject folder')
parser.add_argument('-ga','--group_a',required=False,  help='group a')
parser.add_argument('-gb','--group_b',required=False,  help='group b')
parser.add_argument('-d','--delimiter',required=False,  help='delimiter for classification file')
parser.add_argument('-l','--lenght',required=False,  help='minimum lenght')
parser.add_argument('-std','--std',required=False,  help='standard deviation of clusters')
parser.add_argument('-pt','--plot_groups',required=False,  help='groups for plot')
parser.add_argument('-t','--thickness',required=False,  help='analyze thickness')


args = parser.parse_args()

if args.function == "GA":
	columns=  np.asarray(args.classification_columns.split(":"), dtype=np.int)
	print(columns, columns[0])
	group_a = np.asarray(args.group_a.split(":"), dtype=np.int)
	group_b =np.asarray(args.group_b.split(":"), dtype=np.int)
	clusters_to_analyze= np.asarray(args.clusters_to_analyze.split(":"), dtype=np.int)
		
	#groups=np.asarray(args.plot_groups.split(":"), dtype=np.int)
	groups=[[999999999,0],[1],[2]]
	print(groups)

	eval(args.function)(args.classification_file, columns, args.target_subject, args.subjects_folder,group_a, group_b,args.lenght, args.std, clusters_to_analyze, args.model, args.delimiter,groups, args.thickness )
elif args.function == "thicknessPerStructure":
	columns=  np.asarray(args.classification_columns.split(":"), dtype=np.int)
	print(columns, columns[0])
	group_a = np.asarray(args.group_a.split(":"), dtype=np.int)
	group_b =np.asarray(args.group_b.split(":"), dtype=np.int)
	clusters_to_analyze= np.asarray(args.clusters_to_analyze.split(":"), dtype=np.int)
		
	#groups=np.asarray(args.plot_groups.split(":"), dtype=np.int)
	groups=[[999999999,0],[1],[2]]
	print(groups)

	eval(args.function)(args.classification_file, columns, args.target_subject, args.subjects_folder,group_a, group_b,args.lenght, args.std, clusters_to_analyze, args.model, args.delimiter,groups )


elif args.function == "averageCorrespondingClusters":
	corresponding_folders =  args.corresponding_file_list.replace("]","").replace("[","").split(",")
	images_folders =  args.imagesFolder.replace("]","").replace("[","").split(",")
	cluster_indeces =  np.asarray(args.clusterIndeces.replace("]","").replace("[","").split(","),dtype=np.int)
	eval(args.function)(corresponding_folders, images_folders, args.outputFolder,cluster_indeces)


"""ts="BAKP64e_nrecon-trio3_vb17"
#classFile="/space/snoke/1/public/vivros/labels.csv"
classFile="/autofs/space/snoke_001/public/vivros/hd_tracula/labels.csv"
folder="/autofs/space/snoke_001/public/vivros/hd_tracula/"
cta=[200]

indices = groupAnalysis(headers=["meanFA"],cols=[2], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=cta,target_subject=ts,subjects_dir=folder, delimiter=" ", groupA=[8], groupB=[6])
indices = indices + groupAnalysis(headers=["meanMD"],cols=[6], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=cta,target_subject=ts,subjects_dir=folder, delimiter=" ", groupA=[8], groupB=[6])
indices = indices + groupAnalysis(headers=["meanRD"],cols=[10], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=cta,target_subject=ts,subjects_dir=folder, delimiter=" ", groupA=[8], groupB=[6])
indices = indices + groupAnalysis(headers=["meanAD"],cols=[14], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=cta,target_subject=ts,subjects_dir=folder, delimiter=" ", groupA=[8], groupB=[6])


plotAverageMeasures(headers=["meanFA"],cols=[2], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=indices,target_subject=ts,subjects_dir=folder, delimiter=" ", groups=[[8],[6]])
plotAverageMeasures(headers=["meanMD"],cols=[6], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=indices,target_subject=ts,subjects_dir=folder, delimiter=" ", groups=[[8],[6]])
plotAverageMeasures(headers=["meanRD"],cols=[10], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=indices,target_subject=ts,subjects_dir=folder, delimiter=" ", groups=[[8],[6]])
plotAverageMeasures(headers=["meanAD"],cols=[14], groups_classification=classFile, classification_columns=[0,1],clustersToAnalyze=indices,target_subject=ts,subjects_dir=folder, delimiter=" ", groups=[[8],[6]])

"""

